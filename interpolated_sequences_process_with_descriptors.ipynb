{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5 \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets in c:\\users\\jahaz\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\jahaz\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from PyWavelets) (1.24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\jahaz\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Config\"\"\"\n",
    "discart_points_face = True\n",
    "landmarks = 75 # 75 sin puntos de la cara y con puntos de la cara son 543\n",
    "data = pd.read_csv(\"train.csv\") # Cargar el csv\n",
    "word = \"time\" \n",
    "words = data[\"sign\"].unique()\n",
    "number_words = 33 # Número de palabras a generar\n",
    "filtered_data = data[data[\"sign\"] == word] # Filtrar por palabra\n",
    "csv_path = \"filter_for_word.csv\" # Guardar el csv filtrado\n",
    "filtered_data.to_csv(csv_path, index=False) # Guardar el csv filtrado\n",
    "num_frames = 60 # Número de fotogramas para las secuencias interpoladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leer un .parquet aleatorio\n",
    "path_one_file = filtered_data.iloc[6][\"path\"]\n",
    "df = pd.read_parquet(path_one_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path_data(path_number, filtered_data):\n",
    "    \"\"\"\n",
    "    Procesa los datos de una ruta específica (los .parquet), extrae y reformatea la información relevante de los puntos (x, y).\n",
    "\n",
    "    Args:\n",
    "        path_number (int): El índice de la ruta en el DataFrame filtered_data.\n",
    "        filtered_data (pandas.DataFrame): Un DataFrame que contiene información sobre las rutas de los archivos de datos.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Un array de la secuencia reformateada con la forma (num_samples, 543, 2), donde num_samples es el\n",
    "                       número de fotogramas únicos en los datos.\n",
    "    \"\"\"\n",
    "    path_file = filtered_data.iloc[path_number][\"path\"]\n",
    "    data = pd.read_parquet(path_file, columns=[\"frame\", \"x\", \"y\", \"type\"])\n",
    "    if discart_points_face:\n",
    "        data = data[data[\"type\"] != \"face\"]\n",
    "    # Reemplazar NaN con 0\n",
    "    cleaned_data = np.nan_to_num(data[[\"x\", \"y\"]].to_numpy())\n",
    "    num_samples = data[\"frame\"].nunique()\n",
    "    seq = cleaned_data.reshape(num_samples, landmarks, 2)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sequence(seq, num_frames):\n",
    "    \"\"\"\n",
    "    Interpola una secuencia única a una longitud específica en la dimensión del tiempo.\n",
    "    \n",
    "    Args:\n",
    "        seq (numpy.ndarray): La secuencia de entrada con la forma (T, landmarks, 2), donde T es el número de fotogramas.\n",
    "        num_frames (int): El número de fotogramas para la secuencia interpolada.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: La secuencia interpolada con la forma (num_frames, landmarks, 2).\n",
    "    \"\"\"\n",
    "    seq_interp = np.zeros((num_frames, landmarks, 2))\n",
    "    seq = np.nan_to_num(seq)\n",
    "\n",
    "    def interp_func(x):\n",
    "        f = interp1d(np.arange(seq.shape[0]), x, kind='linear', bounds_error=False)\n",
    "        return f(np.linspace(0, seq.shape[0] - 1, num_frames))\n",
    "\n",
    "    seq_interp[:,:,0] = np.apply_along_axis(interp_func, 0, seq[:,:,0])\n",
    "    seq_interp[:,:,1] = np.apply_along_axis(interp_func, 0, seq[:,:,1])\n",
    "\n",
    "    return seq_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sequences_list(sequences, num_frames):\n",
    "    \"\"\"\n",
    "    Interpola una lista de secuencias a una longitud específica en la dimensión del tiempo.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list): Una lista de secuencias, cada una con la forma (T, 543, 2), donde T es el número de fotogramas.\n",
    "        num_frames (int): El número de fotogramas para las secuencias interpoladas.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Un array que contiene las secuencias interpoladas con la forma (num_frames, 543, 2, num_sequences),\n",
    "                       donde num_sequences es el número de secuencias en la lista de entrada.\n",
    "    \"\"\"\n",
    "    num_sequences = len(sequences)\n",
    "    interpolated_sequences = np.zeros((num_frames, landmarks, 2, num_sequences))\n",
    "    \n",
    "    for idx, seq in enumerate(sequences):\n",
    "        seq_interp = interpolate_sequence(seq, num_frames)\n",
    "        interpolated_sequences[:, :, :, idx] = seq_interp\n",
    "    \n",
    "    return interpolated_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "def add_descriptors(sequence):\n",
    "    \"\"\"\n",
    "    Args: for example: (60, 75, 2) where 60 is the number of frames, 75 is the number of landmarks and 2 is the x and y coordinates\n",
    "    Returns: (60, 75, 1) 1 is for the resume X and Y coordinates in one value mean of the wavelet coefficients\n",
    "    \"\"\"\n",
    "    descriptors = np.mean(pywt.dwt(sequence, 'db1', axis=-1)[0], axis=-1, keepdims=True)\n",
    "    return descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_file_train(filtered_data, num_frames):\n",
    "    \"\"\"unic word train data select in the config\"\"\"\n",
    "    sequences = [] # Lista de secuencias\n",
    "    for i in range(filtered_data.shape[0]):\n",
    "        seq = process_path_data(i, filtered_data)\n",
    "        sequences.append(seq)\n",
    "\n",
    "    interpolated_sequences = interpolate_sequences_list(sequences, num_frames)\n",
    "    interpolated_sequences = interpolated_sequences.transpose((3,0,1,2)) #reorganizacion\n",
    "\n",
    "    # Añade los descriptores de derivada para cada secuencia interpolada\n",
    "    interpolated_sequences_with_descriptors = []\n",
    "    for seq in interpolated_sequences:\n",
    "        seq_with_descriptors = add_descriptors(seq)\n",
    "        interpolated_sequences_with_descriptors.append(seq_with_descriptors)\n",
    "\n",
    "    # Convierte la lista de secuencias interpoladas con descriptores en un array de NumPy\n",
    "    interpolated_sequences_with_descriptors = np.array(interpolated_sequences_with_descriptors)\n",
    "\n",
    "    print(interpolated_sequences_with_descriptors.shape) # (num_sequences, num_frames, landmarks, n-descriptors)\n",
    "\n",
    "    \"\"\" export interpolated_sequences to npy\"\"\"\n",
    "    if discart_points_face is True:\n",
    "        np.save(f\"interpolated_sequences_{word}_75points_with_descriptors.npy\", interpolated_sequences_with_descriptors)\n",
    "        plt.plot(interpolated_sequences_with_descriptors[1, :, 0], interpolated_sequences_with_descriptors[1, :, 1], 'o')\n",
    "    else:\n",
    "        np.save(f\"interpolated_sequences_{word}_543points_with_descriptors.npy\", interpolated_sequences_with_descriptors)\n",
    "        plt.plot(interpolated_sequences_with_descriptors[1, :, 0], interpolated_sequences_with_descriptors[1, :, 1], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palabra vacuum tiene 307 muestras\n",
      "(307, 60, 75, 1)\n",
      "palabra mouth tiene 391 muestras\n",
      "(391, 60, 75, 1)\n",
      "palabra tomorrow tiene 388 muestras\n",
      "(388, 60, 75, 1)\n",
      "palabra cloud tiene 378 muestras\n",
      "(378, 60, 75, 1)\n",
      "palabra cut tiene 369 muestras\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m filtered_data \u001b[39m=\u001b[39m data[data[\u001b[39m\"\u001b[39m\u001b[39msign\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m word] \u001b[39m# Filtrar por palabra\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpalabra\u001b[39m\u001b[39m\"\u001b[39m, word, \u001b[39m\"\u001b[39m\u001b[39mtiene\u001b[39m\u001b[39m\"\u001b[39m, filtered_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mmuestras\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m export_file_train(filtered_data, num_frames)\n",
      "Cell \u001b[1;32mIn[74], line 5\u001b[0m, in \u001b[0;36mexport_file_train\u001b[1;34m(filtered_data, num_frames)\u001b[0m\n\u001b[0;32m      3\u001b[0m sequences \u001b[39m=\u001b[39m [] \u001b[39m# Lista de secuencias\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(filtered_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m----> 5\u001b[0m     seq \u001b[39m=\u001b[39m process_path_data(i, filtered_data)\n\u001b[0;32m      6\u001b[0m     sequences\u001b[39m.\u001b[39mappend(seq)\n\u001b[0;32m      8\u001b[0m interpolated_sequences \u001b[39m=\u001b[39m interpolate_sequences_list(sequences, num_frames)\n",
      "Cell \u001b[1;32mIn[70], line 14\u001b[0m, in \u001b[0;36mprocess_path_data\u001b[1;34m(path_number, filtered_data)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mProcesa los datos de una ruta específica (los .parquet), extrae y reformatea la información relevante de los puntos (x, y).\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m                   número de fotogramas únicos en los datos.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m path_file \u001b[39m=\u001b[39m filtered_data\u001b[39m.\u001b[39miloc[path_number][\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(path_file, columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mframe\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m discart_points_face:\n\u001b[0;32m     16\u001b[0m     data \u001b[39m=\u001b[39m data[data[\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mface\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 509\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[0;32m    510\u001b[0m     path,\n\u001b[0;32m    511\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m    512\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    513\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[0;32m    514\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    515\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    516\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parquet.py:227\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    221\u001b[0m     path,\n\u001b[0;32m    222\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    223\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m    224\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     pa_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mread_table(\n\u001b[0;32m    228\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    229\u001b[0m     )\n\u001b[0;32m    230\u001b[0m     result \u001b[39m=\u001b[39m pa_table\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    232\u001b[0m     \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\parquet\\core.py:2973\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2962\u001b[0m         \u001b[39m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   2963\u001b[0m         dataset \u001b[39m=\u001b[39m ParquetFile(\n\u001b[0;32m   2964\u001b[0m             source, metadata\u001b[39m=\u001b[39mmetadata, read_dictionary\u001b[39m=\u001b[39mread_dictionary,\n\u001b[0;32m   2965\u001b[0m             memory_map\u001b[39m=\u001b[39mmemory_map, buffer_size\u001b[39m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2970\u001b[0m             thrift_container_size_limit\u001b[39m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   2971\u001b[0m         )\n\u001b[1;32m-> 2973\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39;49mread(columns\u001b[39m=\u001b[39;49mcolumns, use_threads\u001b[39m=\u001b[39;49muse_threads,\n\u001b[0;32m   2974\u001b[0m                         use_pandas_metadata\u001b[39m=\u001b[39;49muse_pandas_metadata)\n\u001b[0;32m   2976\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2977\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPassing \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to get the legacy behaviour is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2978\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2979\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbe removed in a future version.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2980\u001b[0m     \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m   2982\u001b[0m \u001b[39mif\u001b[39;00m ignore_prefixes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\parquet\\core.py:2601\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   2593\u001b[0m         index_columns \u001b[39m=\u001b[39m [\n\u001b[0;32m   2594\u001b[0m             col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   2595\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, \u001b[39mdict\u001b[39m)\n\u001b[0;32m   2596\u001b[0m         ]\n\u001b[0;32m   2597\u001b[0m         columns \u001b[39m=\u001b[39m (\n\u001b[0;32m   2598\u001b[0m             \u001b[39mlist\u001b[39m(columns) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(index_columns) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(columns))\n\u001b[0;32m   2599\u001b[0m         )\n\u001b[1;32m-> 2601\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset\u001b[39m.\u001b[39;49mto_table(\n\u001b[0;32m   2602\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filter_expression,\n\u001b[0;32m   2603\u001b[0m     use_threads\u001b[39m=\u001b[39;49muse_threads\n\u001b[0;32m   2604\u001b[0m )\n\u001b[0;32m   2606\u001b[0m \u001b[39m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m \u001b[39m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   2608\u001b[0m \u001b[39mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"main\"\"\"\n",
    "# se seleccionan de forma aleatoria y de un tamaño de \"number_words\" las palabras que se van a utilizar de forma tal de que no se repitan\n",
    "words_select = np.random.choice(words, size=number_words, replace=False)\n",
    "for word in words_select:\n",
    "    filtered_data = data[data[\"sign\"] == word] # Filtrar por palabra\n",
    "    print(\"palabra\", word, \"tiene\", filtered_data.shape[0], \"muestras\")\n",
    "    export_file_train(filtered_data, num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "def process_word(word):\n",
    "    filtered_data = data[data[\"sign\"] == word]\n",
    "    export_file_train(filtered_data, num_frames)\n",
    "\n",
    "# Seleccionar palabras de forma aleatoria y no repetirlas\n",
    "words_select = np.random.choice(words, size=number_words, replace=False)\n",
    "\n",
    "# Crear una barra de progreso para las palabras\n",
    "with tqdm(total=len(words_select)) as pbar:\n",
    "    \n",
    "    # Crear una cola de palabras para procesar\n",
    "    words_queue = mp.Queue()\n",
    "    for word in words_select:\n",
    "        words_queue.put(word)\n",
    "    \n",
    "    # Crear un grupo de procesos\n",
    "    num_processes = mp.cpu_count()\n",
    "    process_pool = mp.Pool(num_processes)\n",
    "    \n",
    "    # Procesar las palabras en paralelo\n",
    "    while not words_queue.empty():\n",
    "        # Obtener palabras de la cola y procesarlas en paralelo\n",
    "        batch = [words_queue.get() for _ in range(num_processes)]\n",
    "        process_pool.map(process_word, batch)\n",
    "        pbar.update(len(batch))\n",
    "    \n",
    "    # Cerrar el grupo de procesos\n",
    "    process_pool.close()\n",
    "    process_pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 60, 75, 4)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"all words selects train data select in the config\"\"\"\n",
    "sequences = [] # Lista de secuencias\n",
    "for i in range(filtered_data.shape[0]):\n",
    "    seq = process_path_data(i, filtered_data)\n",
    "    sequences.append(seq)\n",
    "\n",
    "interpolated_sequences = interpolate_sequences_list(sequences, num_frames)\n",
    "interpolated_sequences = interpolated_sequences.transpose((3,0,1,2)) #reorganizacion\n",
    "\n",
    "# Añade los descriptores de derivada para cada secuencia interpolada\n",
    "interpolated_sequences_with_descriptors = []\n",
    "for seq in interpolated_sequences:\n",
    "    seq_with_descriptors = add_descriptors(seq)\n",
    "    interpolated_sequences_with_descriptors.append(seq_with_descriptors)\n",
    "\n",
    "# Convierte la lista de secuencias interpoladas con descriptores en un array de NumPy\n",
    "interpolated_sequences_with_descriptors = np.array(interpolated_sequences_with_descriptors)\n",
    "\n",
    "print(interpolated_sequences_with_descriptors.shape) # (num_sequences, num_frames, landmarks, n-descriptors)\n",
    "\n",
    "\"\"\" export interpolated_sequences to npy\"\"\"\n",
    "if discart_points_face is True:\n",
    "    np.save(f\"interpolated_sequences_{word}_75points_with_descriptors.npy\", interpolated_sequences_with_descriptors)\n",
    "    plt.plot(interpolated_sequences_with_descriptors[1, :, 0], interpolated_sequences_with_descriptors[1, :, 1], 'o')\n",
    "else:\n",
    "    np.save(f\"interpolated_sequences_{word}_543points_with_descriptors.npy\", interpolated_sequences_with_descriptors)\n",
    "    plt.plot(interpolated_sequences_with_descriptors[1, :, 0], interpolated_sequences_with_descriptors[1, :, 1], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'interpolated_sequences_time_75points_with_descriptors.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m data_descriptors_rendering \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minterpolated_sequences_\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m_75points_with_descriptors.npy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m val\u001b[39m=\u001b[39m[]\n\u001b[1;32m----> 6\u001b[0m time  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39minterpolated_sequences_time_75points_with_descriptors.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m puzzle\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39minterpolated_sequences_puzzle_75points_with_descriptors.npy\u001b[39m\u001b[39m'\u001b[39m  )\n\u001b[0;32m      9\u001b[0m \u001b[39m# %% Matriz de correlación\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'interpolated_sequences_time_75points_with_descriptors.npy'"
     ]
    }
   ],
   "source": [
    "\"\"\" visualizar datos\"\"\"\n",
    "\n",
    "data_descriptors_rendering = np.load(f\"interpolated_sequences_{word}_75points_with_descriptors.npy\")\n",
    "\n",
    "val=[]\n",
    "time  = np.load('interpolated_sequences_time_75points_with_descriptors.npy')\n",
    "puzzle= np.load('interpolated_sequences_puzzle_75points_with_descriptors.npy'  )\n",
    "\n",
    "# %% Matriz de correlación\n",
    "for sec in range(0,20):\n",
    "    M1=time[sec,:,:,0]\n",
    "    M2=puzzle[sec,:,:,0]\n",
    "    # Transponemos los datos para que los puntos sean las filas y los fotogramas sean las columnas\n",
    "    M1 = np.transpose(M1, (1, 0))\n",
    "    M2 = np.transpose(M2, (1, 0))\n",
    "    # Unimos las dos matrices a lo largo de la dimensión 0\n",
    "    matrices = np.concatenate((M1, M2), axis=1)\n",
    "    # Calculamos la matriz de correlación utilizando numpy.corrcoef()\n",
    "    corr_matrix = np.corrcoef(matrices)\n",
    "    #corr_matrix = np.where(np.isnan(corr_matrix), 0, corr_matrix)\n",
    "    plt.figure()\n",
    "    plt.imshow(corr_matrix,cmap='jet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99478e5ee0f521ef0b070dd453c82598403954a17ba545340915d72841086dea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
